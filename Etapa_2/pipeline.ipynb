{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\volpi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\volpi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\volpi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\volpi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "import re, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "import joblib\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv('fake_news_spanish.csv', sep = ';', encoding = 'utf-8')\n",
    "data = datos.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word is not None:\n",
    "          new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "          new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word is not None:\n",
    "            new_word = word.lower()\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "            \n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word is not None:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "   p = inflect.engine()\n",
    "   new_words = []\n",
    "   for word in words:\n",
    "       if word.isdigit():\n",
    "           new_word = p.number_to_words(word)\n",
    "           new_words.append(new_word)\n",
    " \n",
    "       else:\n",
    "           new_words.append(word)\n",
    "   return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            new_words.append(w)\n",
    "    return new_words\n",
    "\n",
    "def preprocessing(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(words):\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "   stems = stem_words(words)\n",
    "   lemmas = lemmatize_verbs(words)\n",
    "   return stems + lemmas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento + Tokenizacion + Normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesamiento_completo(data):\n",
    "    data.dropna()\n",
    "    data['Titulo'] = data['Titulo'].fillna(' ')\n",
    "    data = data.drop_duplicates(subset = ['Titulo', 'Descripcion'], keep = 'first')\n",
    "    data['Fecha'] = pd.to_datetime(data['Fecha'], errors='coerce')\n",
    "    data = data.drop(columns='ID')\n",
    "    data['Descripcion'] = data['Descripcion'].apply(contractions.fix)\n",
    "    data['Titulo'] = data['Titulo'].apply(contractions.fix)\n",
    "    data['words_descripcion'] = data['Descripcion'].apply(word_tokenize)\n",
    "    data['words_titulo'] = data['Titulo'].apply(word_tokenize) \n",
    "    data['words_descripcion'].dropna()\n",
    "    data['words_titulo'].dropna()\n",
    "    data['prep_descripcion'] = data['words_descripcion'].apply(preprocessing)\n",
    "    data['prep_titulo'] = data['words_titulo'].apply(preprocessing)\n",
    "    data['prep_descripcion'] = data['prep_descripcion'].apply(stem_and_lemmatize) \n",
    "    data['prep_titulo'] = data['prep_titulo'].apply(stem_and_lemmatize)\n",
    "    data['prep_descripcion'] = data['prep_descripcion'].apply(lambda x: ' '.join(map(str, x)))\n",
    "    data['prep_titulo'] = data['prep_titulo'].apply(lambda x: ' '.join(map(str, x)))\n",
    "    data[\"concatenado\"] = data[\"prep_titulo\"] + \" \" + data[\"prep_descripcion\"]\n",
    "    \n",
    "    return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\volpi\\AppData\\Local\\Temp\\ipykernel_28568\\63361374.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Fecha'] = pd.to_datetime(data['Fecha'], errors='coerce')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Descripcion</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>words_descripcion</th>\n",
       "      <th>words_titulo</th>\n",
       "      <th>prep_descripcion</th>\n",
       "      <th>prep_titulo</th>\n",
       "      <th>concatenado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>'The Guardian' va con Sánchez: 'Europa necesit...</td>\n",
       "      <td>El diario británico publicó este pasado jueves...</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>[El, diario, británico, publicó, este, pasado,...</td>\n",
       "      <td>['The, Guardian, ', va, con, Sánchez, :, 'Euro...</td>\n",
       "      <td>diario britanico publico pasado juev edit prox...</td>\n",
       "      <td>the guard va sanchez europ necesit apuest frut...</td>\n",
       "      <td>the guard va sanchez europ necesit apuest frut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>[REVELAN, QUE, EL, GOBIERNO, NEGOCIO, LA, LIBE...</td>\n",
       "      <td>[REVELAN, QUE, EL, GOBIERNO, NEGOCIO, LA, LIBE...</td>\n",
       "      <td>revel gobierno negocio liberac mirel cambio ot...</td>\n",
       "      <td>revel gobierno negocio liberac mirel cambio ot...</td>\n",
       "      <td>revel gobierno negocio liberac mirel cambio ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>El 'Ahora o nunca' de Joan Fuster sobre el est...</td>\n",
       "      <td>El valencianismo convoca en Castelló su fiesta...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>[El, valencianismo, convoca, en, Castelló, su,...</td>\n",
       "      <td>[El, 'Ahora, o, nunca, ', de, Joan, Fuster, so...</td>\n",
       "      <td>valencianismo convoc castello fiest grand conm...</td>\n",
       "      <td>ahor nunc joan fust estatuto valenciano cumpl ...</td>\n",
       "      <td>ahor nunc joan fust estatuto valenciano cumpl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Iglesias alienta a Yolanda Díaz, ERC y EH Bild...</td>\n",
       "      <td>En política, igual que hay que negociar con lo...</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>[En, política, ,, igual, que, hay, que, negoci...</td>\n",
       "      <td>[Iglesias, alienta, a, Yolanda, Díaz, ,, ERC, ...</td>\n",
       "      <td>politic ig negoci empresario negoci grupo parl...</td>\n",
       "      <td>iglesia alient yoland diaz erc eh bildu negoci...</td>\n",
       "      <td>iglesia alient yoland diaz erc eh bildu negoci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Puigdemont: 'No sería ninguna tragedia una rep...</td>\n",
       "      <td>En una entrevista en El Punt Avui, el líder de...</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>[En, una, entrevista, en, El, Punt, Avui, ,, e...</td>\n",
       "      <td>[Puigdemont, :, 'No, sería, ninguna, tragedia,...</td>\n",
       "      <td>entrevist punt avu lid jxcat desdramatizado po...</td>\n",
       "      <td>puigdemont ser ningun traged repet elecc puigd...</td>\n",
       "      <td>puigdemont ser ningun traged repet elecc puigd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57058</th>\n",
       "      <td>1</td>\n",
       "      <td>El Defensor del Pueblo reclama a la Comunidad ...</td>\n",
       "      <td>El gobierno regional han indicado que la atenc...</td>\n",
       "      <td>2021-08-06</td>\n",
       "      <td>[El, gobierno, regional, han, indicado, que, l...</td>\n",
       "      <td>[El, Defensor, del, Pueblo, reclama, a, la, Co...</td>\n",
       "      <td>gobierno reg indicado at dia incluyendo at inm...</td>\n",
       "      <td>defens pueblo reclam comunidad madrid dato dem...</td>\n",
       "      <td>defens pueblo reclam comunidad madrid dato dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57059</th>\n",
       "      <td>0</td>\n",
       "      <td>El EQUO plantea ceder la presidencia de la Com...</td>\n",
       "      <td>Si la higiene democrática nos lleva a esa exig...</td>\n",
       "      <td>2020-08-09</td>\n",
       "      <td>[Si, la, higiene, democrática, nos, lleva, a, ...</td>\n",
       "      <td>[El, EQUO, plantea, ceder, la, presidencia, de...</td>\n",
       "      <td>si higy democratic llev exigenc ten pas person...</td>\n",
       "      <td>equo plante ced presidenc comunidad madrid cs ...</td>\n",
       "      <td>equo plante ced presidenc comunidad madrid cs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57060</th>\n",
       "      <td>1</td>\n",
       "      <td>Alberto Garzón: 'Que los Borbones son unos lad...</td>\n",
       "      <td>El coordinador federal de IU asegura que la mo...</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>[El, coordinador, federal, de, IU, asegura, qu...</td>\n",
       "      <td>[Alberto, Garzón, :, 'Que, los, Borbones, son,...</td>\n",
       "      <td>coordinad fed iu asegur monarqu putrefact coor...</td>\n",
       "      <td>alberto garzon borbon ladron hecho historica c...</td>\n",
       "      <td>alberto garzon borbon ladron hecho historica c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57061</th>\n",
       "      <td>1</td>\n",
       "      <td>Vox exige entrar en el Gobierno de Castilla y ...</td>\n",
       "      <td>Santiago Abascal: Vox tiene el derecho y el de...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>[Santiago, Abascal, :, Vox, tiene, el, derecho...</td>\n",
       "      <td>[Vox, exige, entrar, en, el, Gobierno, de, Cas...</td>\n",
       "      <td>santiago abasc vox derecho deb form gobierno c...</td>\n",
       "      <td>vox exig entr gobierno castill leon car vicepr...</td>\n",
       "      <td>vox exig entr gobierno castill leon car vicepr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57062</th>\n",
       "      <td>1</td>\n",
       "      <td>Unas 300 personas protestan contra la visita d...</td>\n",
       "      <td>Los Mossos dEsquadra han blindado los alrededo...</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>[Los, Mossos, dEsquadra, han, blindado, los, a...</td>\n",
       "      <td>[Unas, 300, personas, protestan, contra, la, v...</td>\n",
       "      <td>mosso desquadr blindado alr estac franc barcel...</td>\n",
       "      <td>una three hundred persona protest visit rey ba...</td>\n",
       "      <td>una three hundred persona protest visit rey ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56613 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label                                             Titulo  \\\n",
       "0          1  'The Guardian' va con Sánchez: 'Europa necesit...   \n",
       "1          0  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...   \n",
       "2          1  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
       "3          1  Iglesias alienta a Yolanda Díaz, ERC y EH Bild...   \n",
       "4          0  Puigdemont: 'No sería ninguna tragedia una rep...   \n",
       "...      ...                                                ...   \n",
       "57058      1  El Defensor del Pueblo reclama a la Comunidad ...   \n",
       "57059      0  El EQUO plantea ceder la presidencia de la Com...   \n",
       "57060      1  Alberto Garzón: 'Que los Borbones son unos lad...   \n",
       "57061      1  Vox exige entrar en el Gobierno de Castilla y ...   \n",
       "57062      1  Unas 300 personas protestan contra la visita d...   \n",
       "\n",
       "                                             Descripcion      Fecha  \\\n",
       "0      El diario británico publicó este pasado jueves... 2023-02-06   \n",
       "1      REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ... 2023-01-10   \n",
       "2      El valencianismo convoca en Castelló su fiesta...        NaT   \n",
       "3      En política, igual que hay que negociar con lo... 2022-03-01   \n",
       "4      En una entrevista en El Punt Avui, el líder de... 2018-09-03   \n",
       "...                                                  ...        ...   \n",
       "57058  El gobierno regional han indicado que la atenc... 2021-08-06   \n",
       "57059  Si la higiene democrática nos lleva a esa exig... 2020-08-09   \n",
       "57060  El coordinador federal de IU asegura que la mo... 2018-12-07   \n",
       "57061  Santiago Abascal: Vox tiene el derecho y el de...        NaT   \n",
       "57062  Los Mossos dEsquadra han blindado los alrededo... 2020-09-10   \n",
       "\n",
       "                                       words_descripcion  \\\n",
       "0      [El, diario, británico, publicó, este, pasado,...   \n",
       "1      [REVELAN, QUE, EL, GOBIERNO, NEGOCIO, LA, LIBE...   \n",
       "2      [El, valencianismo, convoca, en, Castelló, su,...   \n",
       "3      [En, política, ,, igual, que, hay, que, negoci...   \n",
       "4      [En, una, entrevista, en, El, Punt, Avui, ,, e...   \n",
       "...                                                  ...   \n",
       "57058  [El, gobierno, regional, han, indicado, que, l...   \n",
       "57059  [Si, la, higiene, democrática, nos, lleva, a, ...   \n",
       "57060  [El, coordinador, federal, de, IU, asegura, qu...   \n",
       "57061  [Santiago, Abascal, :, Vox, tiene, el, derecho...   \n",
       "57062  [Los, Mossos, dEsquadra, han, blindado, los, a...   \n",
       "\n",
       "                                            words_titulo  \\\n",
       "0      ['The, Guardian, ', va, con, Sánchez, :, 'Euro...   \n",
       "1      [REVELAN, QUE, EL, GOBIERNO, NEGOCIO, LA, LIBE...   \n",
       "2      [El, 'Ahora, o, nunca, ', de, Joan, Fuster, so...   \n",
       "3      [Iglesias, alienta, a, Yolanda, Díaz, ,, ERC, ...   \n",
       "4      [Puigdemont, :, 'No, sería, ninguna, tragedia,...   \n",
       "...                                                  ...   \n",
       "57058  [El, Defensor, del, Pueblo, reclama, a, la, Co...   \n",
       "57059  [El, EQUO, plantea, ceder, la, presidencia, de...   \n",
       "57060  [Alberto, Garzón, :, 'Que, los, Borbones, son,...   \n",
       "57061  [Vox, exige, entrar, en, el, Gobierno, de, Cas...   \n",
       "57062  [Unas, 300, personas, protestan, contra, la, v...   \n",
       "\n",
       "                                        prep_descripcion  \\\n",
       "0      diario britanico publico pasado juev edit prox...   \n",
       "1      revel gobierno negocio liberac mirel cambio ot...   \n",
       "2      valencianismo convoc castello fiest grand conm...   \n",
       "3      politic ig negoci empresario negoci grupo parl...   \n",
       "4      entrevist punt avu lid jxcat desdramatizado po...   \n",
       "...                                                  ...   \n",
       "57058  gobierno reg indicado at dia incluyendo at inm...   \n",
       "57059  si higy democratic llev exigenc ten pas person...   \n",
       "57060  coordinad fed iu asegur monarqu putrefact coor...   \n",
       "57061  santiago abasc vox derecho deb form gobierno c...   \n",
       "57062  mosso desquadr blindado alr estac franc barcel...   \n",
       "\n",
       "                                             prep_titulo  \\\n",
       "0      the guard va sanchez europ necesit apuest frut...   \n",
       "1      revel gobierno negocio liberac mirel cambio ot...   \n",
       "2      ahor nunc joan fust estatuto valenciano cumpl ...   \n",
       "3      iglesia alient yoland diaz erc eh bildu negoci...   \n",
       "4      puigdemont ser ningun traged repet elecc puigd...   \n",
       "...                                                  ...   \n",
       "57058  defens pueblo reclam comunidad madrid dato dem...   \n",
       "57059  equo plante ced presidenc comunidad madrid cs ...   \n",
       "57060  alberto garzon borbon ladron hecho historica c...   \n",
       "57061  vox exig entr gobierno castill leon car vicepr...   \n",
       "57062  una three hundred persona protest visit rey ba...   \n",
       "\n",
       "                                             concatenado  \n",
       "0      the guard va sanchez europ necesit apuest frut...  \n",
       "1      revel gobierno negocio liberac mirel cambio ot...  \n",
       "2      ahor nunc joan fust estatuto valenciano cumpl ...  \n",
       "3      iglesia alient yoland diaz erc eh bildu negoci...  \n",
       "4      puigdemont ser ningun traged repet elecc puigd...  \n",
       "...                                                  ...  \n",
       "57058  defens pueblo reclam comunidad madrid dato dem...  \n",
       "57059  equo plante ced presidenc comunidad madrid cs ...  \n",
       "57060  alberto garzon borbon ladron hecho historica c...  \n",
       "57061  vox exig entr gobierno castill leon car vicepr...  \n",
       "57062  una three hundred persona protest visit rey ba...  \n",
       "\n",
       "[56613 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocesamiento_completo(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Vectorización y Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class Transformer_Representacion_Seleccion:\n",
    "    def __init__(self, count_vectorizer):\n",
    "        self.count_vectorizer = count_vectorizer\n",
    "        self.palabras = None\n",
    "        self.palabras_deseadas = None  \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ajustar el CountVectorizer y obtener las palabras (características)\n",
    "        X_transformed = self.count_vectorizer.fit_transform(X)\n",
    "        self.palabras = self.count_vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Crear un DataFrame temporal para realizar las selecciones de palabras relevantes\n",
    "        X_todas_palabras = pd.DataFrame(X_transformed.toarray(), columns=self.palabras)\n",
    "        \n",
    "        # Seleccionar las palabras relevantes usando un ciclo tradicional\n",
    "        self.palabras_deseadas = []\n",
    "        for nombre in X_todas_palabras.columns:\n",
    "            # Contar cuántas filas tienen valores distintos de 0 para cada palabra\n",
    "            dato = (X_todas_palabras[nombre] != 0).sum()\n",
    "            if dato > 1:\n",
    "                # Si la palabra aparece en más de una fila, la agregamos a la lista\n",
    "                self.palabras_deseadas.append(nombre)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Transformar los datos usando CountVectorizer\n",
    "        X_transformed = self.count_vectorizer.transform(X)\n",
    "        \n",
    "        # Crear DataFrame con todas las palabras\n",
    "        X_todas_palabras = pd.DataFrame(X_transformed.toarray(), columns=self.palabras)\n",
    "        \n",
    "        # Seleccionar solo las palabras relevantes\n",
    "        palabras_a_usar = pd.Index(self.palabras_deseadas).intersection(X_todas_palabras.columns)\n",
    "        \n",
    "        # Retornar el DataFrame con las palabras relevantes\n",
    "        return X_todas_palabras[palabras_a_usar].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FunctionTransformer\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocesamiento', FunctionTransformer(preprocesamiento_completo)),\n",
    "    ('representacion', Transformer_Representacion_Seleccion(CountVectorizer())),\n",
    "    ('clasificador', LogisticRegression(C=1, max_iter=1000, solver='saga') )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_preprocessed = preprocesamiento_completo(data)\n",
    "\n",
    "X = data_preprocessed['concatenado']\n",
    "y = data_preprocessed['Label']\n",
    "\n",
    "pipeline.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
